,translated_answer,translated_question,question,answer,qa_pair_score,most_relevant_contexts,context_scores,true_answer,true_answer_similarity_score
0,zero or more occurrences of the immediately previous character or regular expression,What is the meaning of the Kleene star in Regex?,What is the meaning of the Kleene star in Regex?,zero or more occurrences of the immediately previous character or regular expression,"[{'label': 'LABEL_1', 'score': 0.6614015102386475}]","['The wildcard is often used together with the Kleene star to mean ""any string of characters"". For example, suppose we want to find any line in which a particular word, for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.*aardvark/.', 'This language consists of strings with a b, followed by at least two a\'s, followed by an exclamation point. The set of operators that allows us to say things like ""some number of as"" are based on the asterisk or *, commonly called the Kleene * (gen-Kleene * erally pronounced ""cleany star""). The Kleene star means ""zero or more occurrences of the immediately previous character or regular expression"". So /a*/ means ""any string of zero or more as"". This will match a or aaaaaa, but it will also match Off Minor since the string Off Minor has zero a\'s. So the regular expression for matching one or more a is /aa*/, meaning one a followed by zero or more as. More complex patterns can also be repeated. So /[ab]*/ means ""zero or more a\'s or b\'s"" (not ""zero or more right square braces""). This will match strings like aaaa or ababab or bbbb.', 'One very important special character is the period (/./), a wildcard expression that matches any single character (except a carriage return), as shown in Fig. 2 .6.', 'The square braces can also be used to specify what a single character cannot be, by use of the caret ˆ. If the caret ˆ is the first symbol after the open square brace [, the resulting pattern is negated. For example, the pattern /[ˆa]/ matches any single character (including special characters) except a. This is only true when the caret is the first symbol after the open square brace. If it occurs anywhere else, it usually stands for a caret; Fig. 2.4 shows some examples.', 'Finally, certain special characters are referred to by special notation based on the backslash (\\) (see Fig. 2 .10). The most common of these are the newline character newline \\n and the tab character \\t. To refer to characters that are special themselves (like ., *, [, and \\) , precede them with a backslash, (i.e., /\\./, /\\*/, /\\[/, and /\\\\/).']","[0.6157941818237305, 0.4809015691280365, 0.4472956657409668, 0.4467812776565552, 0.44579923152923584]","The Kleene star means ""zero or more occurrences of the immediately previous character or regular expression""",tensor([[0.5945]])
1,"it returns true if pattern does not match, but again is zero-width and doesn","What is the usage of the Regex lookahead operator ""?=""?","What is the usage of the Regex lookahead operator ""?=""?","it returns true if pattern does not match, but again is zero-width and doesn","[{'label': 'LABEL_0', 'score': 0.8391243815422058}]","['These lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn’t advance the cursor. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any single word that doesn’t start with “Volcano”. We can use negative lookahead to do this: /ˆ(?!Volcano)[A-Za-z]+/', 'Note that the $ character has a different function here than the end-of-line function we discussed earlier. Most regular expression parsers are smart enough to realize that $ here doesn’t mean end-of-line. (As a thought experiment, think about how regex parsers might figure out the function of $ from the context.)', 'Suppose we need to search for texts about pets; perhaps we are particularly interested in cats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can\'t use the square brackets to search for ""cat or dog"" (why can\'t we say /[catdog]/?), we need a new operator, the disjunction operator, also disjunction called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or the string dog.', '/the*/ matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, /the|any/ matches the or any but not thany or theny. Patterns can be ambiguous in another way. Consider the expression /[a-z]*/ when matching against the text once upon a time. Since /[a-z]*/ matches zero or more letters, this expression could match nothing, or just the first letter o, on, onc, or once. In these cases regular expressions always match the largest string they can; we say that patterns are greedy, expanding to cover as much of a string as they can. There are, however, ways to enforce non-greedy matching, using another meaning of the ? qualifier. The operator *? is a Kleene star that matches as little text as possible. The operator +? is a Kleene plus that matches as little text as possible.', 'An important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string: s/colour/color/']","[0.4603317379951477, 0.4540523886680603, 0.4458838999271393, 0.44580405950546265, 0.44431108236312866]","The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance.",tensor([[0.6734]])
2,tokenizing (segmenting) words,What are the most common steps in a text normalization process?,What are the most common steps in a text normalization process?,tokenizing (segmenting) words,"[{'label': 'LABEL_1', 'score': 0.9853553771972656}]","['Before almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: 1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences In the next sections we walk through each of these tasks.', ""This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:"", '• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.', 'Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.', 'normalized by the number of words preceding all words, as follows:']","[0.7947356104850769, 0.6900253891944885, 0.5773230791091919, 0.549781084060669, 0.534494936466217]",1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences,tensor([[0.8292]])
3,"token learner, and token segmenter",What are the two most common components of a tokenization scheme?,What are the two most common components of a tokenization scheme?,"token learner, and token segmenter","[{'label': 'LABEL_0', 'score': 0.9882997274398804}]","['One commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets. This standard separates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words together, and separates out all punctuation (to save space we’re showing visible spaces ‘ ’ between tokens, although newlines is a more common output):', '• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.', 'Depending on the application, tokenization algorithms may also tokenize multiword expressions like New York or rock ’n’ roll as a single token, which requires a multiword expression dictionary of some sort. Tokenization is thus intimately tied up with named entity recognition, the task of detecting names, dates, and organizations (Chapter 8).', 'Most tokenization schemes have two parts: a token learner, and a token segmenter. The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens. The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary. Three algorithms are widely used: byte-pair encoding (Sennrich et al., 2016) , unigram language modeling (Kudo, 2018) , and WordPiece (Schuster and Nakajima, 2012) ; there is also a SentencePiece library that includes implementations of the first two of the three (Kudo and Richardson, 2018) .', '• A subword vocabulary consisting of 30,000 tokens generated using the Word-Piece algorithm (Schuster and Nakajima, 2012) , • Hidden layers of size of 768, • 12 layers of transformer blocks, with 12 multihead attention layers each.']","[0.5551068782806396, 0.5148247480392456, 0.4840828776359558, 0.4655061364173889, 0.4333339333534241]","a token learner, and a token segmenter",tensor([[0.9867]])
4,learning a vocabulary,What is the purpose of a token segmenter?,What is the purpose of a token segmenter?,learning a vocabulary,"[{'label': 'LABEL_0', 'score': 0.8945798873901367}]","['The token learner part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.', 'A tokenizer can also be used to expand clitic contractions that are marked by apostrophes, for example, converting what’re to the two tokens what are, and we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only occur when it is attached to another word. Some such contractions occur in other alphabetic languages, including articles and pronouns in French (j’ai, l’homme).', 'One commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets. This standard separates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words together, and separates out all punctuation (to save space we’re showing visible spaces ‘ ’ between tokens, although newlines is a more common output):', '• A subword vocabulary consisting of 30,000 tokens generated using the Word-Piece algorithm (Schuster and Nakajima, 2012) , • Hidden layers of size of 768, • 12 layers of transformer blocks, with 12 multihead attention layers each.', ""However, for Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required. These can also be useful for Chinese word segmentation in the rare situations where word rather than character boundaries are required. The standard segmentation algorithms for these languages use neural sequence models trained via supervised machine learning on hand-segmented training sets; we'll introduce sequence models in Chapter 8 and Chapter 9.""]","[0.4611819088459015, 0.4595140516757965, 0.44555121660232544, 0.4341721534729004, 0.4193059206008911]",The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary.,tensor([[0.3331]])
5,learning a vocabulary,What is the purpose of a token learner in the BPE algorithm?,What is the purpose of a token learner in the BPE algorithm?,learning a vocabulary,"[{'label': 'LABEL_0', 'score': 0.961419939994812}]","['The token learner part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.', ""In this section we introduce the simplest of the three, the byte-pair encoding or BPE algorithm (Sennrich et al., 2016) ; see Fig. 2 .13. The BPE token learner begins BPE with a vocabulary that is just the set of all individual characters. It then examines the training corpus, chooses the two symbols that are most frequently adjacent (say 'A', 'B'), adds a new merged symbol 'AB' to the vocabulary, and replaces every adjacent 'A' 'B' in the corpus with the new 'AB'. It continues to count and merge, creating new longer and longer character strings, until k merges have been done creating k novel tokens; k is thus a parameter of the algorithm. The resulting vocabulary consists of the original set of characters plus k new symbols."", 'The MLM training objective is to predict the original inputs for each of the masked tokens using a bidirectional encoder of the kind described in the last section. The cross-entropy loss from these predictions drives the training process for all the parameters in the model. Note that all of the input tokens play a role in the selfattention process, but only the sampled tokens are used for learning.', 'We gave the BPE algorithm in detail in Chapter 2; here are more details on the wordpiece algorithm, which is given a training corpus and a desired vocabulary size and proceeds as follows:', 'Of course in real algorithms BPE is run with many thousands of merges on a very large input corpus. The result is that most words will be represented as full symbols, and only the very rare words (and unknown words) will have to be represented by their parts.']","[0.8182370662689209, 0.5064473152160645, 0.5024523138999939, 0.4842345118522644, 0.44373512268066406]","taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.",tensor([[0.5454]])
6,the task of putting words/tokens in a standard format,What is word normalization?,What is word normalization?,the task of putting words/tokens in a standard format,"[{'label': 'LABEL_0', 'score': 0.8928318619728088}]","['Before almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: 1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences In the next sections we walk through each of these tasks.', 'Word normalization is the task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms like USA and US or uh-huh and uhhuh. This standardization may be valuable, despite the spelling information that is lost in the normalization process. For information retrieval or information extraction about the US, we might want to see information from documents whether they mention the US or the USA.', '• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.', 'normalized by the number of words preceding all words, as follows:', 'Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.']","[0.7254784107208252, 0.7191673517227173, 0.6906026601791382, 0.6590621471405029, 0.6341896057128906]",Word normalization is the task of putting words/tokens in a standard format,tensor([[0.8122]])
7,lemmatization is performed by removing suffixes from the end of the word,How is lemmatization performed?,How is lemmatization performed?,lemmatization is performed by removing suffixes from the end of the word,"[{'label': 'LABEL_0', 'score': 0.9987190961837769}]","['Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story.', 'Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off word-final affixes. This naive version of morphological analysis is called stemming. One of stemming the most widely used stemming algorithms is the Porter (1980) . The Porter stemmer stemmer applied to the following paragraph:', 'Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.', 'How is lemmatization done? The most sophisticated methods for lemmatization involve complete morphological parsing of the word. Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes. Two broad classes of morphemes can be distinguished: stems-the central morpheme of the word, supplying the main meaning-and affixes-adding ""additional"" meanings of various kinds. So, for example, the word fox consists of one morpheme (the morpheme fox) and the word cats consists of two: the morpheme cat and the morpheme -s. A morphological parser takes a word like cats and parses it into the two morphemes cat and s, or parses a Spanish word like amaren (\'if in the future they would love\') into the morpheme amar \'to love\', and the morphological features 3PL and future subjunctive.', 'How about inflected forms like cats versus cat? These two words have the same lemma cat but are different wordforms. A lemma is a set of lexical forms having lemma the same stem, the same major part-of-speech, and the same word sense. The wordform is the full inflected or derived form of the word. For morphologically complex wordform languages like Arabic, we often need to deal with lemmatization. For many tasks in English, however, wordforms are sufficient.']","[0.6898200511932373, 0.54027259349823, 0.4120367169380188, 0.34882181882858276, 0.34114760160446167]",The most sophisticated methods for lemmatization involve complete morphological parsing of the word.,tensor([[0.7318]])
