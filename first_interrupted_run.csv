,translated_answer,translated_question,question,answer,qa_pair_score,most_relevant_contexts,context_scores,true_answer,true_answer_similarity_score
0,zero or more occurrences of the immediately previous character or regular expression,What is the meaning of the Kleene star in Regex?,What is the meaning of the Kleene star in Regex?,zero or more occurrences of the immediately previous character or regular expression,"[{'label': 'LABEL_1', 'score': 0.6614015102386475}]","['The wildcard is often used together with the Kleene star to mean ""any string of characters"". For example, suppose we want to find any line in which a particular word, for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.*aardvark/.', 'This language consists of strings with a b, followed by at least two a\'s, followed by an exclamation point. The set of operators that allows us to say things like ""some number of as"" are based on the asterisk or *, commonly called the Kleene * (gen-Kleene * erally pronounced ""cleany star""). The Kleene star means ""zero or more occurrences of the immediately previous character or regular expression"". So /a*/ means ""any string of zero or more as"". This will match a or aaaaaa, but it will also match Off Minor since the string Off Minor has zero a\'s. So the regular expression for matching one or more a is /aa*/, meaning one a followed by zero or more as. More complex patterns can also be repeated. So /[ab]*/ means ""zero or more a\'s or b\'s"" (not ""zero or more right square braces""). This will match strings like aaaa or ababab or bbbb.', 'One very important special character is the period (/./), a wildcard expression that matches any single character (except a carriage return), as shown in Fig. 2 .6.', 'The square braces can also be used to specify what a single character cannot be, by use of the caret ˆ. If the caret ˆ is the first symbol after the open square brace [, the resulting pattern is negated. For example, the pattern /[ˆa]/ matches any single character (including special characters) except a. This is only true when the caret is the first symbol after the open square brace. If it occurs anywhere else, it usually stands for a caret; Fig. 2.4 shows some examples.', 'Finally, certain special characters are referred to by special notation based on the backslash (\\) (see Fig. 2 .10). The most common of these are the newline character newline \\n and the tab character \\t. To refer to characters that are special themselves (like ., *, [, and \\) , precede them with a backslash, (i.e., /\\./, /\\*/, /\\[/, and /\\\\/).']","[0.6157941818237305, 0.4809015691280365, 0.4472956657409668, 0.4467812776565552, 0.44579923152923584]","The Kleene star means ""zero or more occurrences of the immediately previous character or regular expression""",tensor([[0.5945]])
1,"it returns true if pattern does not match, but again is zero-width and doesn","What is the usage of the Regex lookahead operator ""?=""?","What is the usage of the Regex lookahead operator ""?=""?","it returns true if pattern does not match, but again is zero-width and doesn","[{'label': 'LABEL_0', 'score': 0.8391243815422058}]","['These lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn’t advance the cursor. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any single word that doesn’t start with “Volcano”. We can use negative lookahead to do this: /ˆ(?!Volcano)[A-Za-z]+/', 'Note that the $ character has a different function here than the end-of-line function we discussed earlier. Most regular expression parsers are smart enough to realize that $ here doesn’t mean end-of-line. (As a thought experiment, think about how regex parsers might figure out the function of $ from the context.)', 'Suppose we need to search for texts about pets; perhaps we are particularly interested in cats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can\'t use the square brackets to search for ""cat or dog"" (why can\'t we say /[catdog]/?), we need a new operator, the disjunction operator, also disjunction called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or the string dog.', '/the*/ matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, /the|any/ matches the or any but not thany or theny. Patterns can be ambiguous in another way. Consider the expression /[a-z]*/ when matching against the text once upon a time. Since /[a-z]*/ matches zero or more letters, this expression could match nothing, or just the first letter o, on, onc, or once. In these cases regular expressions always match the largest string they can; we say that patterns are greedy, expanding to cover as much of a string as they can. There are, however, ways to enforce non-greedy matching, using another meaning of the ? qualifier. The operator *? is a Kleene star that matches as little text as possible. The operator +? is a Kleene plus that matches as little text as possible.', 'An important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string: s/colour/color/']","[0.4603317379951477, 0.4540523886680603, 0.4458838999271393, 0.44580405950546265, 0.44431108236312866]","The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance.",tensor([[0.6734]])
2,tokenizing (segmenting) words,What are the most common steps in a text normalization process?,What are the most common steps in a text normalization process?,tokenizing (segmenting) words,"[{'label': 'LABEL_1', 'score': 0.9853553771972656}]","['Before almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: 1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences In the next sections we walk through each of these tasks.', ""This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:"", '• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.', 'Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.', 'normalized by the number of words preceding all words, as follows:']","[0.7947356104850769, 0.6900253891944885, 0.5773230791091919, 0.549781084060669, 0.534494936466217]",1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences,tensor([[0.8292]])
3,"token learner, and token segmenter",What are the two most common components of a tokenization scheme?,What are the two most common components of a tokenization scheme?,"token learner, and token segmenter","[{'label': 'LABEL_0', 'score': 0.9882997274398804}]","['One commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets. This standard separates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words together, and separates out all punctuation (to save space we’re showing visible spaces ‘ ’ between tokens, although newlines is a more common output):', '• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.', 'Depending on the application, tokenization algorithms may also tokenize multiword expressions like New York or rock ’n’ roll as a single token, which requires a multiword expression dictionary of some sort. Tokenization is thus intimately tied up with named entity recognition, the task of detecting names, dates, and organizations (Chapter 8).', 'Most tokenization schemes have two parts: a token learner, and a token segmenter. The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens. The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary. Three algorithms are widely used: byte-pair encoding (Sennrich et al., 2016) , unigram language modeling (Kudo, 2018) , and WordPiece (Schuster and Nakajima, 2012) ; there is also a SentencePiece library that includes implementations of the first two of the three (Kudo and Richardson, 2018) .', '• A subword vocabulary consisting of 30,000 tokens generated using the Word-Piece algorithm (Schuster and Nakajima, 2012) , • Hidden layers of size of 768, • 12 layers of transformer blocks, with 12 multihead attention layers each.']","[0.5551068782806396, 0.5148247480392456, 0.4840828776359558, 0.4655061364173889, 0.4333339333534241]","a token learner, and a token segmenter",tensor([[0.9867]])
4,learning a vocabulary,What is the purpose of a token segmenter?,What is the purpose of a token segmenter?,learning a vocabulary,"[{'label': 'LABEL_0', 'score': 0.8945798873901367}]","['The token learner part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.', 'A tokenizer can also be used to expand clitic contractions that are marked by apostrophes, for example, converting what’re to the two tokens what are, and we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only occur when it is attached to another word. Some such contractions occur in other alphabetic languages, including articles and pronouns in French (j’ai, l’homme).', 'One commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets. This standard separates out clitics (doesn’t becomes does plus n’t), keeps hyphenated words together, and separates out all punctuation (to save space we’re showing visible spaces ‘ ’ between tokens, although newlines is a more common output):', '• A subword vocabulary consisting of 30,000 tokens generated using the Word-Piece algorithm (Schuster and Nakajima, 2012) , • Hidden layers of size of 768, • 12 layers of transformer blocks, with 12 multihead attention layers each.', ""However, for Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required. These can also be useful for Chinese word segmentation in the rare situations where word rather than character boundaries are required. The standard segmentation algorithms for these languages use neural sequence models trained via supervised machine learning on hand-segmented training sets; we'll introduce sequence models in Chapter 8 and Chapter 9.""]","[0.4611819088459015, 0.4595140516757965, 0.44555121660232544, 0.4341721534729004, 0.4193059206008911]",The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary.,tensor([[0.3331]])
5,learning a vocabulary,What is the purpose of a token learner in the BPE algorithm?,What is the purpose of a token learner in the BPE algorithm?,learning a vocabulary,"[{'label': 'LABEL_0', 'score': 0.961419939994812}]","['The token learner part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.', ""In this section we introduce the simplest of the three, the byte-pair encoding or BPE algorithm (Sennrich et al., 2016) ; see Fig. 2 .13. The BPE token learner begins BPE with a vocabulary that is just the set of all individual characters. It then examines the training corpus, chooses the two symbols that are most frequently adjacent (say 'A', 'B'), adds a new merged symbol 'AB' to the vocabulary, and replaces every adjacent 'A' 'B' in the corpus with the new 'AB'. It continues to count and merge, creating new longer and longer character strings, until k merges have been done creating k novel tokens; k is thus a parameter of the algorithm. The resulting vocabulary consists of the original set of characters plus k new symbols."", 'The MLM training objective is to predict the original inputs for each of the masked tokens using a bidirectional encoder of the kind described in the last section. The cross-entropy loss from these predictions drives the training process for all the parameters in the model. Note that all of the input tokens play a role in the selfattention process, but only the sampled tokens are used for learning.', 'We gave the BPE algorithm in detail in Chapter 2; here are more details on the wordpiece algorithm, which is given a training corpus and a desired vocabulary size and proceeds as follows:', 'Of course in real algorithms BPE is run with many thousands of merges on a very large input corpus. The result is that most words will be represented as full symbols, and only the very rare words (and unknown words) will have to be represented by their parts.']","[0.8182370662689209, 0.5064473152160645, 0.5024523138999939, 0.4842345118522644, 0.44373512268066406]","taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.",tensor([[0.5454]])
6,the task of putting words/tokens in a standard format,What is word normalization?,What is word normalization?,the task of putting words/tokens in a standard format,"[{'label': 'LABEL_0', 'score': 0.8928318619728088}]","['Before almost any natural language processing of a text, the text has to be normalized. At least three tasks are commonly applied as part of any normalization process: 1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences In the next sections we walk through each of these tasks.', 'Word normalization is the task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms like USA and US or uh-huh and uhhuh. This standardization may be valuable, despite the spelling information that is lost in the normalization process. For information retrieval or information extraction about the US, we might want to see information from documents whether they mention the US or the USA.', '• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.', 'normalized by the number of words preceding all words, as follows:', 'Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.']","[0.7254784107208252, 0.7191673517227173, 0.6906026601791382, 0.6590621471405029, 0.6341896057128906]",Word normalization is the task of putting words/tokens in a standard format,tensor([[0.8122]])
7,lemmatization is performed by removing suffixes from the end of the word,How is lemmatization performed?,How is lemmatization performed?,lemmatization is performed by removing suffixes from the end of the word,"[{'label': 'LABEL_0', 'score': 0.9987190961837769}]","['Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story.', 'Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off word-final affixes. This naive version of morphological analysis is called stemming. One of stemming the most widely used stemming algorithms is the Porter (1980) . The Porter stemmer stemmer applied to the following paragraph:', 'Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.', 'How is lemmatization done? The most sophisticated methods for lemmatization involve complete morphological parsing of the word. Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes. Two broad classes of morphemes can be distinguished: stems-the central morpheme of the word, supplying the main meaning-and affixes-adding ""additional"" meanings of various kinds. So, for example, the word fox consists of one morpheme (the morpheme fox) and the word cats consists of two: the morpheme cat and the morpheme -s. A morphological parser takes a word like cats and parses it into the two morphemes cat and s, or parses a Spanish word like amaren (\'if in the future they would love\') into the morpheme amar \'to love\', and the morphological features 3PL and future subjunctive.', 'How about inflected forms like cats versus cat? These two words have the same lemma cat but are different wordforms. A lemma is a set of lexical forms having lemma the same stem, the same major part-of-speech, and the same word sense. The wordform is the full inflected or derived form of the word. For morphologically complex wordform languages like Arabic, we often need to deal with lemmatization. For many tasks in English, however, wordforms are sufficient.']","[0.6898200511932373, 0.54027259349823, 0.4120367169380188, 0.34882181882858276, 0.34114760160446167]",The most sophisticated methods for lemmatization involve complete morphological parsing of the word.,tensor([[0.7318]])
8,"the task of determining that two words have the same root, despite their surface differences.",What is lemmatization?,What is lemmatization?,"the task of determining that two words have the same root, despite their surface differences.","[{'label': 'LABEL_0', 'score': 0.7384399771690369}]","['Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story.', 'Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off word-final affixes. This naive version of morphological analysis is called stemming. One of stemming the most widely used stemming algorithms is the Porter (1980) . The Porter stemmer stemmer applied to the following paragraph:', 'Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.', 'How about inflected forms like cats versus cat? These two words have the same lemma cat but are different wordforms. A lemma is a set of lexical forms having lemma the same stem, the same major part-of-speech, and the same word sense. The wordform is the full inflected or derived form of the word. For morphologically complex wordform languages like Arabic, we often need to deal with lemmatization. For many tasks in English, however, wordforms are sufficient.', 'How is lemmatization done? The most sophisticated methods for lemmatization involve complete morphological parsing of the word. Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes. Two broad classes of morphemes can be distinguished: stems-the central morpheme of the word, supplying the main meaning-and affixes-adding ""additional"" meanings of various kinds. So, for example, the word fox consists of one morpheme (the morpheme fox) and the word cats consists of two: the morpheme cat and the morpheme -s. A morphological parser takes a word like cats and parses it into the two morphemes cat and s, or parses a Spanish word like amaren (\'if in the future they would love\') into the morpheme amar \'to love\', and the morphological features 3PL and future subjunctive.']","[0.7414990067481995, 0.524276077747345, 0.4452224671840668, 0.3742144703865051, 0.35497742891311646]","Lemmatization is the task of determining that two words have the same root, despite their surface differences.",tensor([[0.6696]])
9,the minimum edit distance between two strings is defined as the minimum number of operations it takes to edit,How is the minimum edit distance between two strings defined?,How is the minimum edit distance between two strings defined?,the minimum edit distance between two strings is defined as the minimum number of operations it takes to edit,"[{'label': 'LABEL_0', 'score': 0.9988871216773987}]","['• The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.', ""Let's first define the minimum edit distance between two strings. Given two strings, the source string X of length n, and target string Y of length m, we'll define D [i, j] as the edit distance between X[1..i] and Y [1.. j], i.e., the first i characters of X and the first j characters of Y . The edit distance between X and Y is thus D [n, m] ."", 'Again, the fact that these two strings are very similar (differing by only one word) seems like useful evidence for deciding that they might be coreferent. Edit distance gives us a way to quantify both of these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.', 'How do we find the minimum edit distance? We can think of this as a search task, in which we are searching for the shortest path-a sequence of edits-from one string to another.', ""Finally, we'll need to compare words and other strings. We'll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution.""]","[0.9335794448852539, 0.8673880100250244, 0.8399542570114136, 0.74930340051651, 0.7391838431358337]","the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.",tensor([[0.9532]])
10,Consider the set of possible new wordpieces made by concatenating two wordpieces,What is a language model?,What is a language model?,Consider the set of possible new wordpieces made by concatenating two wordpieces,"[{'label': 'LABEL_0', 'score': 0.9913291931152344}]","['Recall that we evaluate language models by examining how well they predict unseen text. Intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words).', 'Language models can also be a tool for generating text for misinformation, phishing, radicalization, and other socially harmful activities (Brown et al., 2020) . McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit.', 'This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.', 'For language modeling, the classes are the words in the vocabulary, soŷ i here means the probability that the model assigns to the correct next word w t :', '(a) Train an n-gram language model on the training corpus, using the current set of wordpieces. (b) Consider the set of possible new wordpieces made by concatenating two wordpieces from the current lexicon. Choose the one new wordpiece that most increases the language model probability of the training corpus.']","[0.660757303237915, 0.6327842473983765, 0.5947139859199524, 0.5758997201919556, 0.53534996509552]",Models that assign probabilities to sequences of words are called language models,tensor([[0.2267]])
11,the fraction of times the word w i appears among all words in all documents of topic,How can we estimate the probability of a word?,How can we estimate the probability of a word?,the fraction of times the word w i appears among all words in all documents of topic,"[{'label': 'LABEL_0', 'score': 0.9941422343254089}]","['To learn the probability P( f i |c), we\'ll assume a feature is just the existence of a word in the document\'s bag of words, and so we\'ll want P(w i |c), which we compute as the fraction of times the word w i appears among all words in all documents of topic c. We first concatenate all documents with category c into one big ""category c"" text. Then we use the frequency of w i in this concatenated document to give a maximum likelihood estimate of the probability:', 'Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:', '1. Choose a vocabulary (word list) that is fixed in advance. 2. Convert in the training set any word that is not in this set (any word) to the unknown word token <UNK> in a text normalization step. 3. Estimate the probabilities for <UNK> from its counts just like any other regular word in the training set.', 'Let\'s begin with the task of computing P(w|h), the probability of a word w given some history h. Suppose the history h is ""its water is so transparent that"" and we want to know the probability that the next word is the:', 'We model the probability that word c is a real context word for target word w as:']","[0.6786948442459106, 0.6456896066665649, 0.6451226472854614, 0.6408520936965942, 0.62589430809021]",from relative frequency counts,tensor([[0.1447]])
12,a hidden Markov chain,What is a Markov model?,What is a Markov model?,a hidden Markov chain,"[{'label': 'LABEL_0', 'score': 0.9988561868667603}]","['Formally, a Markov chain is specified by the following components:', 'A hidden Markov model (HMM) allows us to talk about both observed events hidden Markov model (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. An HMM is specified by the following components:', 'An HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), it computes a probability distribution over possible sequences of labels and chooses the best label sequence.', ""The HMM is based on augmenting the Markov chain. A Markov chain is a model Markov chain that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state. It's as if to predict tomorrow's weather you could examine today's weather but you weren't allowed to look at yesterday's weather. More formally, consider a sequence of state variables q 1 , q 2 , ..., q i . A Markov model embodies the Markov assumption on the probabilities of this sequence: that"", 'an initial probability distribution over states. π i is the probability that the Markov chain will start in state i. Some states j may have π j = 0, meaning that they cannot be initial states. Also, n i=1 π i = 1 A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state:']","[0.6206581592559814, 0.6106089353561401, 0.5986588001251221, 0.5683189630508423, 0.537415087223053]",Markov models are the class of probabilistic models Markov that assume we can predict the probability of some future unit without looking too far into the past.,tensor([[0.3926]])
13,linear interpolation,What technique can be used to estimate n-gram probabilities?,What technique can be used to estimate n-gram probabilities?,linear interpolation,"[{'label': 'LABEL_1', 'score': 0.9265583157539368}]","['In simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the trigram probability P(w n |w n−2 w n−1 ) by mixing together the unigram, bigram, and trigram probabilities, each weighted by a', '• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. • n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate). • n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity. • The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. • Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.', ""The discounting we have been discussing so far can help solve the problem of zero frequency n-grams. But there is an additional source of knowledge we can draw on. If we are trying to compute P(w n |w n−2 w n−1 ) but we have no examples of a particular trigram w n−2 w n−1 w n , we can instead estimate its probability by using the bigram probability P(w n |w n−1 ). Similarly, if we don't have counts to compute P(w n |w n−1 ), we can look to the unigram P(w n )."", 'The n-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a given training corpus. Another implication is that n-grams do a better and better job of modeling the training corpus as we increase the value of N.', 'The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words.']","[0.5885682106018066, 0.5691528916358948, 0.5520919561386108, 0.5434221029281616, 0.5372037887573242]",maximum likelihood estimation or MLE,tensor([[0.0358]])
14,bigram models are more complicated and require more context,What is the difference between bigram and trigram models?,What is the difference between bigram and trigram models?,bigram models are more complicated and require more context,"[{'label': 'LABEL_0', 'score': 0.9990918636322021}]","['In other words, sometimes using less context is a good thing, helping to generalize more for contexts that the model hasn\'t learned much about. There are two ways to use this n-gram ""hierarchy"". In backoff, we use the trigram if the evidence is backoff sufficient, otherwise we use the bigram, otherwise the unigram. In other words, we only ""back off"" to a lower-order n-gram if we have zero evidence for a higher-order n-gram. By contrast, in interpolation, we always mix the probability estimates from interpolation all the n-gram estimators, weighing and combining the trigram, bigram, and unigram counts.', ""Some practical issues: Although for pedagogical purposes we have only described bigram models, in practice it's more common to use trigram models, which condition on the previous two words rather than the previous word, or 4-gram or even 5-gram models, when there is sufficient training data. Note that for these larger n-grams, we'll need to assume extra contexts to the left and right of the sentence end. For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the first trigram (i.e., P(I|<s><s>)."", 'In simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the trigram probability P(w n |w n−2 w n−1 ) by mixing together the unigram, bigram, and trigram probabilities, each weighted by a', 'The bigram model, for example, approximates the probability of a word given all the previous words P(w n |w 1:n−1 ) by using only the conditional probability of the preceding word P(w n |w n−1 ). In other words, instead of computing the probability', 'In a slightly more sophisticated version of linear interpolation, each λ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the λ s for those trigrams higher and thus give that trigram more weight in the interpolation. Equation 3.28 shows the equation for interpolation with context-conditioned weights:']","[0.5070269107818604, 0.492308109998703, 0.4895547330379486, 0.4701557159423828, 0.44139689207077026]",condition on the previous two words rather than the previous word,tensor([[-0.1059]])
15,if they use identical vocabularies.,How can two probabilisting language models be compared on a test set?,How can two probabilisting language models be compared on a test set?,if they use identical vocabularies.,"[{'label': 'LABEL_0', 'score': 0.5413870811462402}]","[""In practice we don't use raw probability as our metric for evaluating language models, but a variant called perplexity. The perplexity (sometimes called PP for short) perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. For a test set W = w 1 w 2 . . . w N ,:"", 'But what does it mean to ""fit the test set""? The answer is simple: whichever model assigns a higher probability to the test set-meaning it more accurately predicts the test set-is a better model. Given two probabilistic models, the better model is the one that has a tighter fit to the test data or that better predicts the details of the test data, and hence will assign a higher probability to the test data.', 'In this chapter, we\'ll begin exploring the RNN and transformer architectures through the lens of probabilistic language models, so let\'s briefly remind ourselves of the framework for language modeling. Recall from Chapter 3 that probabilistic language models predict the next word in a sequence given some preceding context. For example, if the preceding context is ""Thanks for all the"" and we want to know how likely the next word is ""fish"" we would compute:', 'Recall that we evaluate language models by examining how well they predict unseen text. Intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words).', 'Note that in computing perplexities, the n-gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artificially low. The perplexity of two language models is only comparable if they use identical vocabularies.']","[0.6412659287452698, 0.5367025136947632, 0.5252885818481445, 0.5138706564903259, 0.508092999458313]",whichever model assigns a higher probability to the test set-meaning it more accurately predicts the test set-is a better model,tensor([[0.2464]])
16,the inverse probability of the test set,What is the perplexity of a language model?,What is the perplexity of a language model?,the inverse probability of the test set,"[{'label': 'LABEL_0', 'score': 0.9318538308143616}]","['Note that in computing perplexities, the n-gram model P must be constructed without any knowledge of the test set or any prior knowledge of the vocabulary of the test set. Any kind of knowledge of the test set can cause the perplexity to be artificially low. The perplexity of two language models is only comparable if they use identical vocabularies.', ""In practice we don't use raw probability as our metric for evaluating language models, but a variant called perplexity. The perplexity (sometimes called PP for short) perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. For a test set W = w 1 w 2 . . . w N ,:"", ""An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition or machine translation. Nonetheless, because perplexity often correlates with such improvements, it is commonly used as a quick check on an algorithm. But a model's improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model."", 'Recall that we evaluate language models by examining how well they predict unseen text. Intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words).', ""Finally, let's look at an example of how perplexity can be used to compare different n-gram models. We trained unigram, bigram, and trigram grammars on 38 million words (including start-of-sentence tokens) from the Wall Street Journal, using a 19,979 word vocabulary. We then computed the perplexity of each of these models on a test set of 1.5 million words with Eq. 3.16. The table below shows the perplexity of a 1.5 million word WSJ test set according to each of these grammars.""]","[0.6963425874710083, 0.6351802945137024, 0.6234023571014404, 0.5823649764060974, 0.5766159296035767]","perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words.",tensor([[0.5108]])
17,no answer>,How can language models be prevented from assigning zero probability to unknown words?,How can language models be prevented from assigning zero probability to unknown words?,no answer>,"[{'label': 'LABEL_0', 'score': 0.9398353099822998}]","['Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. We can also assign probabilities to entire sequences by using these conditional probabilities in combination with the chain rule:', 'There are two common ways to train the probabilities of the unknown word model <UNK>. The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance:', ""In practice we don't use raw probability as our metric for evaluating language models, but a variant called perplexity. The perplexity (sometimes called PP for short) perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. For a test set W = w 1 w 2 . . . w N ,:"", 'Each of the two columns above instantiates a language model that can assign a probability to the sentence ""I love this fun film"": P(""I love this fun film""|+) = 0.1 × 0.1 × 0.01 × 0.05 × 0.1 = 0.0000005 P(""I love this fun film""|−) = 0.2 × 0.001 × 0.01 × 0.005 × 0.1 = .0000000010', 'The previous section discussed the problem of words whose bigram probability is zero. But what about words we simply have never seen before?']","[0.6428560018539429, 0.6413708925247192, 0.6219311952590942, 0.5913847088813782, 0.5787986516952515]",smoothing or discounting,tensor([[-0.0313]])
18,pruning,What technique can be used to shrink an n-gram language model?,What technique can be used to shrink an n-gram language model?,pruning,"[{'label': 'LABEL_1', 'score': 0.9335383176803589}]","['An n-gram language model can also be shrunk by pruning, for example only storing n-grams with counts greater than some threshold (such as the count threshold of 40 used for the Google n-gram release) or using entropy to prune less-important n-grams (Stolcke, 1998) . Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al. 2007) .', 'This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.', 'How should we deal with this problem when we build n-gram models? One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish. To build a language model for translating legal documents, we need a training corpus of legal documents. To build a language model for a question-answering system, we need a training corpus of questions.', 'The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words.', ""Although with these toolkits it is possible to build web-scale language models using full Kneser-Ney smoothing, Brants et al. (2007) show that with very large language models a much simpler algorithm may be sufficient. The algorithm is called stupid backoff. Stupid backoff gives up the idea of trying to make the language stupid backoff model a true probability distribution. There is no discounting of the higher-order probabilities. If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed (context-independent) weight. This algorithm does not produce a probability distribution, so we'll follow Brants et al. (2007) in referring to it as S:""]","[0.8097999691963196, 0.6217978596687317, 0.5255093574523926, 0.5173783302307129, 0.47357842326164246]",pruning,tensor([[1.0000]])
19,when the probabilities assigned to a Stationary sequence are invariant with respect to shift,When is a stochastic process said to be stationary?,When is a stochastic process said to be stationary?,when the probabilities assigned to a Stationary sequence are invariant with respect to shift,"[{'label': 'LABEL_0', 'score': 0.9661997556686401}]","['Again, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process:', ""A stochastic process is said to be stationary if the probabilities it assigns to a Stationary sequence are invariant with respect to shifts in the time index. In other words, the probability distribution for words at time t is the same as the probability distribution at time t + 1. Markov models, and hence n-grams, are stationary. For example, in a bigram, P i is dependent only on P i−1 . So if we shift our time index by x, P i+x is still dependent on P i+x−1 . But natural language is not stationary, since as we show in Chapter 12, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent. Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language. To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability. Now we are ready to introduce cross-entropy. The cross-entropy is useful when cross-entropy we don't know the actual probability distribution p that generated some data. It allows us to use some m, which is a model of p (i.e., an approximation to p). The cross-entropy of m on p is defined by"", ""Stochastic gradient descent is called stochastic because it chooses a single random example at a time, moving the weights so as to improve performance on that single example. That can result in very choppy movements, so it's common to compute the gradient over batches of training instances rather than a single instance."", 'Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example, and nudging θ in the right direction (the opposite direction of the gradient). (an ""online algorithm"" is one that processes its input example by example, rather than waiting until it sees the entire input). x is the set of training inputs', 'Formally, a Markov chain is specified by the following components:']","[0.4607822597026825, 0.3858277201652527, 0.3848760724067688, 0.3542807996273041, 0.34284740686416626]",A stochastic process is said to be stationary if the probabilities it assigns to a Stationary sequence are invariant with respect to shifts in the time index.,tensor([[0.7163]])
20,because estimating the probability of every possible combination of features would require huge numbers of parameters and im,Why are simplifying assumptions needed when using a Naive Bayes Classifier?,Why are simplifying assumptions needed when using a Naive Bayes Classifier?,because estimating the probability of every possible combination of features would require huge numbers of parameters and im,"[{'label': 'LABEL_0', 'score': 0.9782356023788452}]","['It is important to avoid harms that may result from classifiers, harms that exist both for naive Bayes classifiers and for the other classification algorithms we introduce in later chapters.', 'Unfortunately, Eq. 4.6 is still too hard to compute directly: without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets. Naive Bayes classifiers therefore make two simplifying assumptions.', 'Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the classĉ which has the maximum posterior probability given the document. In Eq. 4.1 we use the hat notationˆto mean ""our estimate of the correct class"".', 'The final equation for the class chosen by a naive Bayes classifier is thus:', ""The second is commonly called the naive Bayes assumption: this is the condi-naive Bayes assumption tional independence assumption that the probabilities P( f i |c) are independent given the class c and hence can be 'naively' multiplied as follows:""]","[0.7093966007232666, 0.6958275437355042, 0.6944726705551147, 0.6839500665664673, 0.6771999597549438]","without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets.",tensor([[0.7053]])
21,add-one (Laplace) smoothing,What can be done to optimize Naive Bayes when an insufficient amount of labeled data is present?,What can be done to optimize Naive Bayes when an insufficient amount of labeled data is present?,add-one (Laplace) smoothing,"[{'label': 'LABEL_0', 'score': 0.9249223470687866}]","[""Despite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets (Ng and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there's no optimization step). So it's still a reasonable approach to use in some situations."", 'Unfortunately, Eq. 4.6 is still too hard to compute directly: without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets. Naive Bayes classifiers therefore make two simplifying assumptions.', 'to make a classification decision —like naive Bayes and also logistic regression—', 'To apply the naive Bayes classifier to text, we need to consider word positions, by simply walking an index through every word position in the document:', 'But since naive Bayes naively multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence! The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization:P']","[0.5586993098258972, 0.5458903908729553, 0.5106940269470215, 0.5086640119552612, 0.4977254867553711]","derive the positive and negative word features from sentiment lexicons, lists of words that are pre-sentiment lexicons annotated with positive or negative sentiment.",tensor([[-0.0399]])
22,binarized features,What type of features can be used to train a Naive Bayes classifier?,What type of features can be used to train a Naive Bayes classifier?,binarized features,"[{'label': 'LABEL_0', 'score': 0.5852330923080444}]","[""In the previous section we pointed out that naive Bayes doesn't require that our classifier use all the words in the training data as features. In fact features in naive Bayes can express any property of the input text we want."", 'Unfortunately, Eq. 4.6 is still too hard to compute directly: without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets. Naive Bayes classifiers therefore make two simplifying assumptions.', 'to make a classification decision —like naive Bayes and also logistic regression—', 'It is important to avoid harms that may result from classifiers, harms that exist both for naive Bayes classifiers and for the other classification algorithms we introduce in later chapters.', ""• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.""]","[0.727729082107544, 0.6897422075271606, 0.6595452427864075, 0.6515998840332031, 0.613821804523468]","dictionaries, URLs, email addresses, network features, phrases",tensor([[0.1789]])
23,accuracy is not a good metric when the goal is to discover something that is rare or,Why is accuracy rarely used alone for unbalanced text classification tasks?,Why is accuracy rarely used alone for unbalanced text classification tasks?,accuracy is not a good metric when the goal is to discover something that is rare or,"[{'label': 'LABEL_0', 'score': 0.9708147048950195}]","['while the other 999,900 are tweets about something completely unrelated. Imagine a simple classifier that stupidly classified every tweet as ""not about pie"". This classifier would have 999,900 true negatives and only 100 false negatives for an accuracy of 999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should be happy with this classifier? But of course this fabulous \'no pie\' classifier would be completely useless, since it wouldn\'t find a single one of the customer comments we are looking for. In other words, accuracy is not a good metric when the goal is to discover something that is rare, or at least not completely balanced in frequency, which is a very common situation in the world.', 'The goal of classification is to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes. One method for classifying text is to use handwritten rules. There are many areas of language processing where handwritten rule-based classifiers constitute a state-ofthe-art system, or at least part of it.', 'The training and testing procedure for text classification follows what we saw with language modeling (Section 3.2): we use the training set to train the model, then use the development test set (also called a devset) to perhaps tune some parameters, and in general decide what the best model is. Once we come up with what we think is the best model, we run it on the (hitherto unseen) test set to report its performance.', 'While humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on character-or word-overlap and those based on embedding similarity.', ""• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.""]","[0.4932141602039337, 0.44002604484558105, 0.4157196283340454, 0.4142651855945587, 0.4027450680732727]",because accuracy doesn't work well when the classes are unbalanced,tensor([[0.4526]])
